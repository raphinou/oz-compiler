%http://vim-latex.sourceforge.net/documentation/latex-suite.html#environment-mappings
%http://www.latextemplates.com/template/masters-doctoral-thesis
\documentclass[a4paper]{memoir}
\usepackage[utf8]{inputenc}
\usepackage{pgf}
\usepackage[margin=false,inline=true]{fixme}
\usepackage{listings} % DO NOT USE DRAFT DOCUMENT AS IT WILL NOT DISPLAY THE CODE


\fxsetup{theme=color}
\definecolor{fxnote}{rgb}{0.8000,0.0000,0.0000}
\author{Bauduin RaphaÃ«l}
\title{Oz Compiler}
\begin{document}
% configure listings package to handle code as Oz code
\lstset{language=Oz}


% requires memoir
% include git rev
\ifdraftdoc
\makeoddhead{plain}{}{}{\textit{Draft: \today{} Rev: \GITAbrHash{} \VCModifiedText{}}}{}
\fi

% write and require version control info
\immediate\write18{sh ./vc -m} 
\input{vc}
\maketitle
\tableofcontents


\chapter{Introduction}
\section{Initial State}
New vm developed.
Old compiler patched, boot compiler in scala.
Need for a new Oz compiler, written in OZ

\section{Goal and Scope}
Flexible documented and tested compiler taking as input the AST in the form of Oz records, and generating opcodes send to the assembler.
Document as much as possible the whole compilation chain from parser through AST to opcodes and virtual machine.

\chapter{Infrastructure}
\section{Virtual Machine}
Describe virtual machine. I think to put it first so all concepts are introduced by the time I describe the opcodes.
\subsection{Registers}
Y are locals(y(index)), X are work registers (x(index)), G are globals filled sequentially with arrayFill (g(index)), k are constants (k(value))
\subsection{Abstractions}
Abstraction (with G regs)  -> CodeArea (with K regs) -> Code
\section{Compiler input}\label{section:compilerinput}
Describe records received from parser.
Not all will be described here as an exhaustive list is available at %http://www.mozart-oz.org/home/doc/compiler/node7.html#appendix.syntax. 
\subsection{Position in source code}
Most records have the corresponding position in the source code available in their last feature, in the form of a record of the form pos(file linebegin columnbegin fileend lineend columend). This information has to trickle through all transformations so meaningful error messages can be given to the programmer in case of error.
\subsection{Basic data types}
The basic data type described in this section are the simplest node found in the AST as they have no children and are the leafs of the tree.
These are the types that are currently handled, with their corresponding records in the AST:
\begin{description}
  \item[integers] fInt(Val Pos)
  \item[floats] fFloat(Val Pos)
  \item[atoms] fAtom(value position)
\end{description}

\subsection{Variables}
A variable is found in a record fVar(VarName Pos)

\subsection{Unification}
A unification is found in a record fEq(LHS RHS Pos).
A=3 gives fEq(fVar(A \_) fInt(3 \_))

\subsection{Instructions sequence}
A sequence of instructions is wrapped in fAnd records, the first feature being usually one instruction, the second feature being a fAnd if more than one instruction follows,  or a single instruction. The code also handles the case when the two features are fAnd records. %FIXME: is this needed? 
Example:\\
\begin{tabular}{ p{0.3\textwidth} p{0.8\textwidth} }
  \begin{lstlisting}
    A=1
    B=2
    C=3
  \end{lstlisting}
&
  \footnotesize{
  \begin{verbatim}
  fAnd( fEq (fVar (A _) fInt (1 _))
        fAnd(fEq (fVar(B _)) fInt(2 _) 
             fEq(fVar(C _) fInt(3 _))))
  \end{verbatim}
  }
\end{tabular}

\subsection{Declarations}
Declarations are found in records of the form fLocal(Declarations Body Pos), where declarations and Body are both AST subtrees. 
example:

\begin{lstlisting}
local
  A B C
in
  A=1
end
\end{lstlisting}
will give
\begin{verbatim}
fLocal(
       fAnd(
            fVar(A _) 
            fAnd(
                 fVar(B _) 
                 fVar(C _) 
                 _) 
            _) 
       fEq( 
            fVar(A _) 
            fInt(1 _) ))
\end{verbatim}

\subsection{Procedures}
\subsubsection{Definitions}
Procedures are found in records of the form fProc(Name Arguments Body Flags Pos), where the Arguments and Body features are AST subtrees. %FIXME: what can be Name? fVar, $. Other possibilities?
Example:

\subsubsection{Calls}
%FIXME: distinguish arguments and parameters?
Procedure calls are found in records of the form fApply(Proc Arguments Pos). 
Example:

\begin{lstlisting}
  {Compute 1 2 3}
\end{lstlisting}

will result in the AST portion:
\begin{verbatim}
fApply(
       fVar(Compute _)
       |(fInt(1 _)
         |(fInt( 2 _)
           |fInt( 3 _))))
\end{verbatim}

\subsection{Functions}                                                         
Functions are simply procedures that implicitly return exactly one value. This similarity between functions and procedures can be seen in their similar AST for definitions and calls. This similarity will be exploited later on to merge both forms of AST into one.
\subsubsection{Definitions}
Function definitions are found in records fFun(Name Arguments Body Flags Pos). The features are identical to the procedure definition.
\subsubsection{Calls}
The calls of functions are identical to the call of procedures as they are found in fApply.
\subsection{Arithmetic operators}
Arithmetic operators are parsed as the operator application to arguments being the operands. For example
5+2 is parsed as:
\begin{lstlisting}
      fOpApply(
         '+'
         |(
            fInt(5 pos("test2.oz" 1 3 "test2.oz" 1 4) )
            |(
               fInt(2 pos("test2.oz" 1 5 "test2.oz" 1 6) )
               
            )
         )
         \_
      )

\end{lstlisting}
\subsection{Expression marker \$}
In Oz, statements can be transformed in expressions by the use of the \$ marker. This marker is found in the AST as a record fDollar() with a unique feature: it position.

\subsection{Cells}
\subsubsection{Assignation}
fColonEquals(Cell Val Pos) %FIXME
\subsubsection{Read}
fAt(Cell Pos) % FIXME
\section{Compiler output}
Describe opcodes.



\chapter{Compiler}
\section{Architecture}
The AST received from the parser goes through several transformations. These transformations are implemented by functions taking as only argument that they need to transform, and they return the transformed AST.
These functions all have the same structure:
\begin{figure}[h]
\begin{lstlisting}
   fun {Transform AST}
      fun {TransformInt AST Params}
         case AST
         of ... then
         else
            {DefaultPass AST TransformInt Params}
         end
      end
      InitialParams = ...
   in
      {TransformInt AST InitialParams}
   end
\end{lstlisting}
\caption{Structure of a transformation function}
\end{figure}
Having the same structure enables the use of the function DefaultPass, which calls the function passed as second arguments recursively on the AST with Params as last argument, but returns the AST unchanged. This facilitates the recursive calls for default behaviour on elements not modified by the transformation.
Params if a record with label params, and features are added according to use. For an example, look at the globaliser below.%FIXME: add reference
\subsection{Namer}
replaces fVar by fSym. In declarations, creates a new symbol for declared variables, in body, looks at already defined symbols.
fLocal introduces a new environment, in which the variables declared in its first feature are added to the environment, possibly erasing variables with the same name coming from the parent environment.
fProc and fFun do the same, because they implicitely declare their formal parameters % see test 029
It is this manipulation of the environments that make that the code in Figure~\ref{fig:namer_nested_locals} does not throws an error of impossible unification and shows 6.
\begin{figure}
\begin{lstlisting}
local
   A=5
in
   local 
      A=6
   in
      {Show A}
   end
end
\end{lstlisting}
\caption{Nested locals handled by Namer}
\label{fig:namer_nested_locals}
\end{figure}

\subsection{Desugar}\label{section:desugar}
The desugar pass handles the transformation of syntactic sugar code in its canonical form.
The desugar function has two local functions defined, one for handling expressions, one for handling statements.
\subsubsection{On statements and expressions}
Oz distinguishes two groups of instructions: statements and expressions. An expression is ``syntactic sugar for a sequence of operations that return a value'' . Statements do not return a value.
%FIXME: Give references: Concepts, Techniques and Models of Computer Programming, p81.
Handling expressions and statements is often very similar, but sometime require significant differences of treatment. Hence the use two different local functions in the Desugar pass: one for each type of instruction.

From this raises the question of categorising an instruction as a statement or an expression. And this is not as straightforward as one may think, as the context in which the instruction is used can change the character from statement to expression. This is illustrated by this example.
% FIXME: storage element?? Find better name.
In Oz, a cell is a mutable storage element which is created with an initial value (\lstinline!C={NewCell 0}!), it can be given a new value (\lstinline!C:=NewValue!) and its value can be read (\lstinline!@C!). There's also an exchange operation \lstinline!{Exchange C Old New}! which ``atomically binds Old with the old content of the cell, and set New to be the new content''.%FIXME: reference pvr book page 415.
Assigning a new value to a cell is usually a statement returning no value, but when used as the right hand side of an assignment, it becomes an expression, whose value is the Old value of the cell !
\begin{figure}[h]
\begin{lstlisting}
local
  C={NewCell 0}
  Old
in
  C:=1        % statement
  Old=(C:=2)  % expression, whose value is assigned to Old
  {Show Old}  % displays 1
end

\end{lstlisting}
\caption{Cell assignment as statement and as expression}
\label{fig:statement_and_expression1}
\end{figure}
Another illustration is the instruction \lstinline!local ... in ... end! which can be a statement as in Figure \ref{fig:statement_and_expression1}, but also an expression as in the code snippet in Figure \ref{fig:statement_and_expression2}

\begin{figure}[h]
\begin{lstlisting}
   A = local C=2 in 2*C end
   {Show A}
\end{lstlisting}
\caption{local \ldots in \ldots end as an expression}
\label{fig:statement_and_expression2}
\end{figure}

Determining if an instruction is a statement or and expression is done
recursively from top to bottom, the starting case being that the program is a
statement, the ending case being that variables, constants, base operators,
\ldots are expressions.  In Figure \ref{fig:statement_and_expression1}, the
whole code snippet which we consider in this case as a complete program is a
statement. This \lstinline!local...in...end! instruction is thus a statement.
The declaration part of this instruction is always a statement. The body is
also a statement because the whole local...in...end is a statement. If it were
an expression (as in Figure \ref{fig:statement_and_expression2}), the body
would have been an expression! In our case, the body is a sequence of
instructions, resulting in a hierarchy of fAnd records in the AST. A sequence
can be a statement, as in this case, or an expression, in which case the value
of the whole sequence is the value of the last expression. This illustrates the
need of distinct functions when statement and expressions must be handled
differently. The fact is that the character (statement or
expression) of a sequence of instructions is determined at the start of the
sequence, the top of the fAnd hierarchy in the AST. But the impact of this
distinction is found at the very bottom of this hierarchy: the last
instruction, the end of the sequence. To be able to distingish both cases, we
need to branch as soon as we determine the instruction's character.

If we note F the pass of the compiler, FExpr the locally defined function handling expressions, and fStat corresponding function for statements, we can ensure that the right function is applied to each instructions'AST as defined in section \ref{section:compilerinput} by following these rules of identification, and call FStat on statements and FExpr on expressions:

%FIXME possibly replace the AST record by the Oz instruction they represent
\begin{description}
  \item[program] the program is a statement
  \item[fLocal] 
    \begin{itemize}
      \item declarations are always statements
      \item the body has the same character as its fLocal
    \end{itemize}
  \item[fAnd] 
    \begin{itemize}
      \item the first feature is a statement
      \item same character as the fAnd
    \end{itemize}
  \item[fEq] both sides are expressions
  \item[fProc] if the proc is an expression, its identifier must be the marker \$, else its identifier is an expression. The arguments are expressions, and the body is a statement.
  \item[fFun] are similar to fProc
  \item[fApply] calls can be statements or expressions, but in both cases the callee and its arguments are all expressions. 
  \item[fColonEquals] can be statement or expression, but Cell and Val are in both cases expressions.
  \item[fAt(Cell Pos)] is an expression. Its Cell is an expression
  \item[fSym] can be statements (eg in declarations) and expressions (outside of declarations)
  \item[fConst] are expressions
\end{description}



\subsubsection{Arithmetic operators}
Infix arithmetic operators are transformed in the call of their respective function.
The operators are parsed as fOpApply( Operator ArgsList)

This code

\begin{lstlisting}
  5+2
\end{lstlisting}

is parsed as
\begin{verbatim}
      fOpApply(
         '+'
         |(
            fInt(5 _ )
            |(
               fInt(2 _ )
               
            )
         )
         _
      )
\end{verbatim}
and must be transformed in:
\begin{verbatim}
      fApply(
         Number.'+'
         |(
            fInt(5 _ )
            |(
               fInt(2 _ )
               
            )
         )
         _
      )
\end{verbatim}

\subsubsection{Functions}
Functions are transformed in their canonical procedure form. This is done simply by replacing the function by a procedure with exactly the same characteristics, except that it takes one additional argument (the return value), and the body of this procedure is the unification of this new argument with the original body of the function. This resulting AST is itself recursively ``desugared''.

For example, the code
\begin{lstlisting}
   fun {F A B}
      A+B
   end

\end{lstlisting} 
is parsed as (in a redacted form to be readable):
\begin{verbatim}
      fFun(
         fSym(F _)
         |(
            fSym(A _)
            |(
               fSym(B _)
               
            )
         )
         fOpApply(
            '+'
            |(
               fSym(A)
               |(
                  fSym(B)
                  
               )
            )
            _
         )
         
         _
      )
\end{verbatim}
and transformed by the desugar pass to become:
\begin{verbatim}
     fProc(     
         fSym(F)
         |(    
            fSym(A)
            |(
               fSym(B)
               |(
                  fSym(R)
                  
               )
            )
         )
         fEq(
            fSym(R)
            fApply(
               fConst('+' _)
               |(
                  fSym(A _)
                  |(
                     fSym(B _)

                  )
               )
               _
            )
            _
         )

         _
      )

\end{verbatim}

We see that a new symbol, that we denoted R for convenience, is added to the arguments list of the procedure, and this new symbol is then unified with the body of the original function. This unification will in turn be handled by the unnester, which is the next pass of the compiler.

\subsubsection{Transformation in an expression}
A statement can be transformed in an expression of the value assigned to the variable in which place a \$ was placed.
Currently only the form fun \{\$ Args\} is supported
\subsection{Unnester}            
The unnester will result in an AST in which there are only elementary instructions. The possible instructions are
\begin{description}
  \item[Unification] of the form $E_1=E_2$
  \item[Procedure calls] {$E_0$ \ldots $E_n$} \fxnote{Show braces in this code}
\end{description}
\subsection{Unification}
If both sides are elementary, no change is done.

If no side is elementary, one new synthetic variable is declared and unified with each side sequentialy, first with the left-hand side, then with the right-hand side. 

If only one side is elementary a new, we look at what the none elementary side is, and act accordingly:
\begin{description}
  \item[Procedure call] the unification is removed, and the variable target of the assignation is put as last argument of the call
  \item[local D in E end] gets replaced by local D in X=E end
  \item[fAnd(S1 E2)] replaced by fAnd(S1 X=E2)
\end{description}
We need recursive calls, as is illustrated by the following example, also showing why this code works
\begin{lstlisting}
  3+4 = {Show}
\end{lstlisting}
It is transformed due to both sides being non-elementary, in this form in a first step:
\begin{lstlisting}
  local
     T
  in
     T = 3+4
     T = {Show}
  end
\end{lstlisting}
and finally in this form by the transformation of the assignment of a procedure call in the samem procedure call with the return variable passed as additional argument:
\begin{lstlisting}
  local
     T
  in
     T = 3+4
     {Show T}
  end
\end{lstlisting}
The order of the assignments of the first step explains why this doesn't work:
\begin{lstlisting}
  {Show} = 3+4
\end{lstlisting}


\subsection{Globaliser}            
This pass handles the global variables of a procedure, that is, variables used by a procedure that does not declare it (implying that the declaration is done at an upper level. It is interesting to note that the concept of global variables is only defined for procedures.
Just as locals attached an environment to their body, procedures attach their own unique procId to their arguments, declarations and body.
Variables declared by a procedure, including formal parameters, get assigned the procId of said procedure.

Special attention has to be paid to nested procedure declarations. For example
\begin{lstlisting}
A=1
proc {P1}                 % A is global to P1
   B=2
   P2
in
   proc {P2}              % A and B are globals to P2
      P3 
   in
      proc {P3}           % A and B are globals to P3
         C
      in
         {Show A}
         {Show B}
         {Show C}
      end
      {P3}
   end
   {P2}
end
\end{lstlisting}

The globals of procedures in the code have to be determined from the inside to the outside: determine the globals of the deepest nested procedure, and go up. 
We see that the globals of a procedure are:
\begin{itemize}
  \item all variables it uses directly (i.e. not in a procedure definition)
  \item plus the globals of all the procedures it defines
  \item minus the variables it declares itself
\end{itemize}

In the example we see that P3 uses 2 variables declared by another procedure: A and B. 
P2 does not directly use any variable, but it defines P3 which itself has 2 global variables. Those two variables are thus also globals to P2
P1 also does not use any variable directly, but it gets 2 globals from P2, of which it declares one: B. P1 indeed only has one global: A.

In the abstract syntax tree, we replace symbols corresponding to a global variables by a new local symbol of type 'newlocal' referencing the symbol for this variables in the parent proc which might itself be a 'newlocal' symbol, and so on until we reach the level where the variables is declared.

In the abstract syntax tree, fProc is replaced by fDefineProc taking 1 additional argument: the newly created local variables referencing a variables in the surrounding procedure. This additional information will enable the code generator handle global variables.

Handling globals correctly requires to handle numerous cases, and determining the algorithm to use requires some care, as illustrated by these examples:

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
   end
end
\end{lstlisting}
In this case, A is global to P3 and it creates a new local symbol that will reference a symbol local to P2. But for P2, A is also global, and this situation needs to trigger a new local creation in P2, which will be referenced by the new local in P3. Same thing in P1.

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      {Show A}  % Use A before a defined procedure needs it
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
   end
end
\end{lstlisting}

In this case, the traversal of the AST will first analyse {Show A} and create a new local symbol for the variables A in P1. P2 is visited later and will create a new local to reference a symbol local to P2, which is also a new local as A is global to P1. But this new local has already been created, and this situation must not trigger the creation of a new local symbol for A. Rather, the new local symbol to P2 must reference the new local symbol to P1. We see that the new local variables created in a procedure definition may have to be modified by its parent procedure.

Of course, the inverse has to be handled too, i.e. a new local is created for a defined proc, and reused later:

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
      {Show A}  % Use A after a nested proc triggered the creation of a new local.
   end
end
\end{lstlisting}


Of course, a new local symbol must not be created when it would represent a locally declared variables, as in this case:

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
   end
   {Show A}  % A is defined locally, and the nested procs defined should not trigger the creation of a new local, but P1's new local for A should reference the A declared by the local .. in .. end
end
\end{lstlisting}

Two procs defined at the same level must have have their respective new local symbols reference the same symbol at the parent level (whether it be a new local symbol created at the parent level or the existing symbol of a declared variable at the parent level). In this example, P2 and P3 each create a new local for A and both trigger the creation of a new local in P1 for A. But only one new local should be created in P1 for A, and both locals in P2 and P3 should reference it.


Example V:
\begin{lstlisting}
local
   A
in
   proc {P1}
      P2 P3
   in
      proc {P2}
         {Show A}
      end
      proc {P3}
          {Show A}
      end
      {P2}
      {P3}
   end
   {Show A}  % Use A after a nested proc triggered the creation of a new local.
end
\end{lstlisting}

In the end, the algorithm used is the following.
Each call of the globaliser takes the AST sub tree and an additional Params argument with three fields:
\begin{itemize}
   \item the current procId
   \item a list of global variables already seen in the current procId
   \item a list, each item being the list of new local symbols created for the global with same index in the previous list.
\end{itemize}

The globalise handles the following nodes:
\begin{description}
   \item[fProc] When handling a fProc, the globaliser call gets the informations from the parent in its Params argument. Because we enter a new level of procedure nesting, a NewParams is initialised to be passed to recursive globaliser calls handling children nodes. NewParams is initialised with a new procId and two empty lists. 
      The symbol of the procedure itself gets the procId of its parent, found in Params. The arguments of the procedure get the procId of the currently handled procedure, found in NewParams. At that time the globaliser function is called recursively on the children nodes, with NewParams as additional argument.
      Once all children have been traversed, the list of their globals and their respective newly created locals is found in NewParams.  We have multiple new locals corresponding to one global in the case of 2 siblings procedure definitions referring the same global as in example V. 
      Globals whose procId match the NewParams' procId are ignored, as those are variables that are declared at this level, and the chain of references must stop here for these variables.
      For each remaining global, we look at the list of their locals. 
      If one of these has the procId matching the current level, all other locals are changed to reference this one. This is because this specific symbol has been created by a direct use at this level, and we will use this one as the new localised symbol at this level. Then we push the global on the parent's globals list and add the local symbol to its corresponding list of new locals (those lists are found in Params)
      If none of the locals has the current level's procId, it means the global variables has only been used in nested procedure definitions. We need to create a new local symbol for that variable at the current level that will be referenced by the existing locals. Finally, we push the global and the new local symbol we created to the parent. 
   \item[fSym] If the symbol in this fSym has the current procId found in NewParams, keep it as is, do not change it! 
      If its procId is different from the current procId, this fSym represents a global variable for the current level. We look in Params if a local variable with the current procId has already been created for this global variable. If yes we reuse it, else We define a new symbol, having the current procId and referencing the symbol initially in fSym. We modify the fSym to use that newly created symbol. Finally, push the global and its local to the parent.
      
\end{description}



\subsection{CodeGen}
\subsubsection{Register allocation}
It is CodeGen that allocates registers. For the moment only Y registers.
When a Symbol of type global is found, it is handled differently than local symbols.
For a procedure definition, we initialise the global variables with arrayFill( y(yindex)). \fxnote{handle multiple levels of nesting}
For a local\ldots in \ldots end, a g() is issued in place of a y(). \fxnote{is this working yet?}

\section{Tests}
\subsection{Helper functions tests}
Some helper functions are tested specifically by Oz code, checking that the result returned by the function is correct. This is simply done by calling the function in Figure~\ref{fig:helperstestsfunction} with the first argument the function call we want to test, and the expected result as second argument as illustrated in Figure~\ref{fig:helperstestsexample} where the function UnWrapFAnd is tested.
The only downside to this approach is that helper functions have to be exported for them to be available in the test code..

\begin{figure}
\begin{lstlisting}
   proc {Equals Result Expected}
      if Result\=Expected then
         %Show error information
         raise unexpectedResult end
      end
      {System.printInfo '.'}
   end
\end{lstlisting}
\label{fig:helperstestsfunction}
\caption{HelpersTests function}
\end{figure}

\begin{figure}
\begin{lstlisting}
   {Equals 
      {Compile.unWrapFAnd fAnd(first fAnd(second third) )}
      first|second|third|nil }
\end{lstlisting}
\label{fig:helperstestsexample}
\caption{Helper test example}
\end{figure}
\subsection{Compiler tests}
Tests are defined by three files each:
\begin{itemize}
  \item the oz code to compile and execute
  \item the expected stdout
  \item the expected stderr
\end{itemize}
All these files are put under the tests/definitions directory.

A TestRunner script in oz loads the code found in the file whose path is passed as argument, parses it, gives the AST to the compiler which returns the opcodes which are assembled and executed. 
A shell script iterates over all tests and for each execute the TestRunner, puts the stdout and stderr in result files under the tests/results directory, compares their content with the expected results and in case of difference can open a diffviewer (like vimdiff).
All test code has a preamble comment describing what case this test covers.
If the test code contains the comment line
\begin{lstlisting}
%-- SKIP TEST-- 
\end{lstlisting}
the test is skipped. This was implemented to be able to run tests even if one of them was temporarily not passing.

Adding a test is very simple:
\begin{enumerate}
  \item put the code to compile and execute in an .oz file under tests/descriptions
  \item put the expected output in a file with same name but with extension .out
  \item put the expected error output in a file with same name but with extension .err
\end{enumerate}
Once this is done, the new test will be included in the next run.
It is also possible to run one individual test. In this case, the test is run even if it is maked as to be skipped when running the full test suite. This helps writing tests for cases needed to be supported in the future, but that should not make the full test suite fail.
\section{Performance}
Possibly include some measure?



\end{document}
