%http://vim-latex.sourceforge.net/documentation/latex-suite.html#environment-mappings
%http://www.latextemplates.com/template/masters-doctoral-thesis
\documentclass[a4paper]{memoir}
\usepackage[utf8]{inputenc}
\usepackage{pgf}
\usepackage[margin=false,inline=true]{fixme}
\usepackage{listings} % DO NOT USE DRAFT DOCUMENT AS IT WILL NOT DISPLAY THE CODE


\fxsetup{theme=color}
\definecolor{fxnote}{rgb}{0.8000,0.0000,0.0000}
\author{Bauduin RaphaÃ«l}
\title{Oz Compiler}
\begin{document}
% configure listings package to handle code as Oz code
\lstset{language=Oz}


% requires memoir
% include git rev
\ifdraftdoc
\makeoddhead{plain}{}{}{\textit{Draft: \today{} Rev: \GITAbrHash{} \VCModifiedText{}}}{}
\fi

% write and require version control info
\immediate\write18{sh ./vc -m} 
\input{vc}
\maketitle
\tableofcontents


\chapter{Introduction}
\section{Initial State}
New vm developed.
Old compiler patched, boot compiler in scala.
Need for a new Oz compiler, written in OZ

\section{Goal and Scope}
The compiler takes as input the AST in the form of Oz records, and generates opcodes sent to the assembler. 
There were four goals set for the compiler:
\begin{enumerate}
  \item compile the whole language so that it can replace the current compiler in Mozart 2
  \item generate quality code, exploiting the capabilities of the virtual machine
  \item easy to understand and modular code
  \item the compiler should also be extensible so that, for example, support for a new instruction can be added without the recompilation of the compiler.
\end{enumerate}
Although the code is extensively commented, this report contributes to the third goal.

\chapter{Infrastructure}
\section{Virtual Machine}
Describe virtual machine. I think to put it first so all concepts are introduced by the time I describe the opcodes.
\subsection{Registers}
Y are locals(y(index)), X are work registers (x(index)), G are globals filled sequentially with arrayFill (g(index)), k are constants (k(value))
\subsection{Abstractions}
Abstraction (with G regs)  -> CodeArea (with K regs) -> Code
\section{Compiler input}\label{section:compilerinput}
Describe records received from parser.
Not all will be described here as an exhaustive list is available at %http://www.mozart-oz.org/home/doc/compiler/node7.html#appendix.syntax. 
\subsection{Position in source code}
Most records have the corresponding position in the source code available in their last feature, in the form of a record of the form pos(file linebegin columnbegin fileend lineend columend). This information has to trickle through all transformations so meaningful error messages can be given to the programmer in case of error.
\subsection{Basic data types}
The basic data type described in this section are the simplest node found in the AST as they have no children and are the leafs of the tree.
These are the types that are currently handled, with their corresponding records in the AST:
\begin{description}
  \item[integers] fInt(Val Pos)
  \item[floats] fFloat(Val Pos)
  \item[atoms] fAtom(value position)
\end{description}

\subsection{Variables}
A variable is found in a record fVar(VarName Pos)

\subsection{Unification}
A unification is found in a record fEq(LHS RHS Pos).
A=3 gives fEq(fVar(A \_) fInt(3 \_))

\subsection{Instructions sequence}
A sequence of instructions is wrapped in fAnd records, the first feature being usually one instruction, the second feature being a fAnd if more than one instruction follows,  or a single instruction. The code also handles the case when the two features are fAnd records. %FIXME: is this needed? 
Example:\\
\begin{tabular}{ p{0.3\textwidth} p{0.8\textwidth} }
  \begin{lstlisting}
    A=1
    B=2
    C=3
  \end{lstlisting}
&
  \footnotesize{
  \begin{verbatim}
  fAnd( fEq (fVar (A _) fInt (1 _))
        fAnd(fEq (fVar(B _)) fInt(2 _) 
             fEq(fVar(C _) fInt(3 _))))
  \end{verbatim}
  }
\end{tabular}

\subsection{Declarations}
Declarations are found in records of the form fLocal(Declarations Body Pos), where declarations and Body are both AST subtrees. 
example:

\begin{lstlisting}
local
  A B C
in
  A=1
end
\end{lstlisting}
will give
\begin{verbatim}
fLocal(
       fAnd(
            fVar(A _) 
            fAnd(
                 fVar(B _) 
                 fVar(C _) 
                 _) 
            _) 
       fEq( 
            fVar(A _) 
            fInt(1 _) ))
\end{verbatim}

\subsection{Procedures}
\subsubsection{Definitions}
Procedures are found in records of the form fProc(Name Arguments Body Flags Pos), where the Arguments and Body features are AST subtrees. %FIXME: what can be Name? fVar, $. Other possibilities?
Example:

\subsubsection{Calls}
%FIXME: distinguish arguments and parameters?
Procedure calls are found in records of the form fApply(Proc Arguments Pos). 
Example:

\begin{lstlisting}
  {Compute 1 2 3}
\end{lstlisting}

will result in the AST portion:
\begin{verbatim}
fApply(
       fVar(Compute _)
       |(fInt(1 _)
         |(fInt( 2 _)
           |fInt( 3 _))))
\end{verbatim}

\subsection{Functions}                                                         
Functions are simply procedures that implicitly return exactly one value. This similarity between functions and procedures can be seen in their similar AST for definitions and calls. This similarity will be exploited later on to merge both forms of AST into one.
\subsubsection{Definitions}
Function definitions are found in records fFun(Name Arguments Body Flags Pos). The features are identical to the procedure definition.
\subsubsection{Calls}
The calls of functions are identical to the call of procedures as they are found in fApply.
\subsection{Arithmetic operators}
Arithmetic operators are parsed as the operator application to arguments being the operands. For example
5+2 is parsed as:
\begin{lstlisting}
      fOpApply(
         '+'
         |(
            fInt(5 pos("test2.oz" 1 3 "test2.oz" 1 4) )
            |(
               fInt(2 pos("test2.oz" 1 5 "test2.oz" 1 6) )
               
            )
         )
         \_
      )

\end{lstlisting}
\subsection{Expression marker \$}
In Oz, statements can be transformed in expressions by the use of the \$ marker. This marker is found in the AST as a record fDollar() with a unique feature: it position.

\subsection{Cells}
\subsubsection{Assignation}
fColonEquals(Cell Val Pos) %FIXME
\subsubsection{Read}
fAt(Cell Pos) % FIXME
\subsection{Records}
Without describing the whole syntax, here is a reminder of the record syntax in Oz code. A record is refined by \lstinline!Label(F1:V1 F2:V2)! where the label and features ($F_i$) can be an atom or a variable, and the values ($V_i$) can in addition to this be the result of a function call. Features can be left out, in which case they are implicitly assigned increasing values from 1.

Records are present in the AST in records with label fRecord. Its label is the first feature, and the second feature is the list of feature/value pairs. If no feature was specified, the entry in the list is simply the value. If a feature was specified, the entry in the list is a record fColon, with the first feature being the feature and second feature being the value. All this will become much clearer with the following examples.

Here is the AST corresponding to the record definition \lstinline!rec(f1:v1 f2:v2)!. We see that the label (rec) is an atom located in an fAtom record. The pairs of features and their respective values are wrapped in fColon.

\begin{lstlisting}
   fRecord(
      fAtom(rec pos)
      |(
         fColon(
            fAtom(f1 pos)
            fAtom(v1 pos)
         )
         |(
            fColon(
               fAtom(f2 pos)
               fAtom(v2 pos)
            )
            nil
         )
      )
   )
\end{lstlisting}

When the feature is not explicitely given, the item in the list of features and their respective values is simply the value. Here is the AST of this record \lstinline!rec(v1 f2:V2)!. The first item in the list is simply an atom, the value of the first feature. Note that the value of the feature f2 is a variable in this case.

\begin{lstlisting}
   fRecord(
      fAtom(rec pos)
      |(
         fAtom(v1 pos)
         |(
            fColon(
               fAtom(f2 pos)
               fVar(V2 pos)
            )
            nil
         )
      )
   )
\end{lstlisting}


\section{Compiler output}
Describe opcodes.



\chapter{Compiler}
\section{Architecture}
The AST received from the parser goes through several transformations. These transformations are implemented by functions taking as only argument that they need to transform, and they return the transformed AST.
These functions all have the same structure:
\begin{figure}[h]
\begin{lstlisting}
   fun {Transform AST}
      fun {TransformInt AST Params}
         case AST
         of ... then
         else
            {DefaultPass AST TransformInt Params}
         end
      end
      InitialParams = ...
   in
      {TransformInt AST InitialParams}
   end
\end{lstlisting}
\caption{Structure of a transformation function}
\end{figure}
Having the same structure enables the use of the function DefaultPass, which calls the function passed as second arguments recursively on the AST with Params as last argument, but returns the AST unchanged. This facilitates the recursive calls for default behaviour on elements not modified by the transformation.
Params if a record with label params, and features are added according to use. For an example, look at the globaliser below.%FIXME: add reference
\subsection{Namer}
replaces fVar by fSym. In declarations, creates a new symbol for declared variables, in body, looks at already defined symbols.
fLocal introduces a new environment, in which the variables declared in its first feature are added to the environment, possibly erasing variables with the same name coming from the parent environment.
fProc and fFun do the same, because they implicitely declare their formal parameters % see test 029
It is this manipulation of the environments that make that the code in Figure~\ref{fig:namer_nested_locals} does not throws an error of impossible unification and shows 6.
\begin{figure}
\begin{lstlisting}
local
   A=5
in
   local 
      A=6
   in
      {Show A}
   end
end
\end{lstlisting}
\caption{Nested locals handled by Namer}
\label{fig:namer_nested_locals}
\end{figure}

\subsection{Desugar}\label{section:desugar}
The desugar pass handles the transformation of syntactic sugar code in its canonical form.
The desugar function has two local functions defined, one for handling expressions, one for handling statements.
\subsubsection{On statements and expressions}
Oz distinguishes two groups of instructions: statements and expressions. An expression is ``syntactic sugar for a sequence of operations that return a value'' . Statements do not return a value.
%FIXME: Give references: Concepts, Techniques and Models of Computer Programming, p81.
Handling expressions and statements is often very similar, but sometime require significant differences of treatment. Hence the use two different local functions in the Desugar pass: one for each type of instruction.

From this raises the question of categorising an instruction as a statement or an expression. And this is not as straightforward as one may think, as the context in which the instruction is used can change the character from statement to expression. This is illustrated by this example.
% FIXME: storage element?? Find better name.
In Oz, a cell is a mutable storage element which is created with an initial value (\lstinline!C={NewCell 0}!), it can be given a new value (\lstinline!C:=NewValue!) and its value can be read (\lstinline!@C!). There's also an exchange operation \lstinline!{Exchange C Old New}! which ``atomically binds Old with the old content of the cell, and set New to be the new content''.%FIXME: reference pvr book page 415.
Assigning a new value to a cell is usually a statement returning no value, but when used as the right hand side of an assignment, it becomes an expression, whose value is the Old value of the cell !
\begin{figure}[h]
\begin{lstlisting}
local
  C={NewCell 0}
  Old
in
  C:=1        % statement
  Old=(C:=2)  % expression, whose value is assigned to Old
  {Show Old}  % displays 1
end

\end{lstlisting}
\caption{Cell assignment as statement and as expression}
\label{fig:statement_and_expression1}
\end{figure}
Another illustration is the instruction \lstinline!local ... in ... end! which can be a statement as in Figure \ref{fig:statement_and_expression1}, but also an expression as in the code snippet in Figure \ref{fig:statement_and_expression2}

\begin{figure}[h]
\begin{lstlisting}
   A = local C=2 in 2*C end
   {Show A}
\end{lstlisting}
\caption{local \ldots in \ldots end as an expression}
\label{fig:statement_and_expression2}
\end{figure}

Determining if an instruction is a statement or and expression is done
recursively from top to bottom, the starting case being that the program is a
statement, the ending case being that variables, constants, base operators,
\ldots are expressions.  In Figure \ref{fig:statement_and_expression1}, the
whole code snippet which we consider in this case as a complete program is a
statement. This \lstinline!local...in...end! instruction is thus a statement.
The declaration part of this instruction is always a statement. The body is
also a statement because the whole local...in...end is a statement. If it were
an expression (as in Figure \ref{fig:statement_and_expression2}), the body
would have been an expression! In our case, the body is a sequence of
instructions, resulting in a hierarchy of fAnd records in the AST. A sequence
can be a statement, as in this case, or an expression, in which case the value
of the whole sequence is the value of the last expression. This illustrates the
need of distinct functions when statement and expressions must be handled
differently. The fact is that the character (statement or
expression) of a sequence of instructions is determined at the start of the
sequence, the top of the fAnd hierarchy in the AST. But the impact of this
distinction is found at the very bottom of this hierarchy: the last
instruction, the end of the sequence. To be able to distingish both cases, we
need to branch as soon as we determine the instruction's character.

If we note F the pass of the compiler, FExpr the locally defined function handling expressions, and fStat corresponding function for statements, we can ensure that the right function is applied to each instructions'AST as defined in section \ref{section:compilerinput} by following these rules of identification, and call FStat on statements and FExpr on expressions:

%FIXME possibly replace the AST record by the Oz instruction they represent
\begin{description}
  \item[program] the program is a statement
  \item[fLocal] 
    \begin{itemize}
      \item the declarations are all just fSym, since the DeclsFlattener has move all other code to the body. These fSyms are neither expressions nor statement, they are just fSym declarations.
      \item the body has the same character as its fLocal
    \end{itemize}
  \item[fAnd] 
    \begin{itemize}
      \item the first feature is a statement
      \item same character as the fAnd
    \end{itemize}
  \item[fEq] both sides are expressions
  \item[fProc] if the proc is an expression, its identifier must be the marker \$, else its identifier is an expression. The arguments are expressions, and the body is a statement.
  \item[fFun] are similar to fProc
  \item[fApply] calls can be statements or expressions, but in both cases the callee and its arguments are all expressions. 
  \item[fColonEquals] can be statement or expression, but Cell and Val are in both cases expressions.
  \item[fAt(Cell Pos)] is an expression. Its Cell is an expression
  \item[fSym] , outside local's declaration part, are expressions
  \item[fConst] are expressions
  \item[fRecord] are expressions
\end{description}



\subsubsection{Arithmetic operators}
Infix arithmetic operators are transformed in the call of their respective function.
The operators are parsed as fOpApply( Operator ArgsList)

This code

\begin{lstlisting}
  5+2
\end{lstlisting}

is parsed as
\begin{verbatim}
      fOpApply(
         '+'
         |(
            fInt(5 _ )
            |(
               fInt(2 _ )
               
            )
         )
         _
      )
\end{verbatim}
and must be transformed in:
\begin{verbatim}
      fApply(
         Number.'+'
         |(
            fInt(5 _ )
            |(
               fInt(2 _ )
               
            )
         )
         _
      )
\end{verbatim}

\subsubsection{Functions}
Functions are transformed in their canonical procedure form. This is done simply by replacing the function by a procedure with exactly the same characteristics, except that it takes one additional argument (the return value), and the body of this procedure is the unification of this new argument with the original body of the function. This resulting AST is itself recursively ``desugared''.

For example, the code
\begin{lstlisting}
   fun {F A B}
      A+B
   end

\end{lstlisting} 
is parsed as (in a redacted form to be readable):
\begin{verbatim}
      fFun(
         fSym(F _)
         |(
            fSym(A _)
            |(
               fSym(B _)
               
            )
         )
         fOpApply(
            '+'
            |(
               fSym(A)
               |(
                  fSym(B)
                  
               )
            )
            _
         )
         
         _
      )
\end{verbatim}
and transformed by the desugar pass to become:
\begin{verbatim}
     fProc(     
         fSym(F)
         |(    
            fSym(A)
            |(
               fSym(B)
               |(
                  fSym(R)
                  
               )
            )
         )
         fEq(
            fSym(R)
            fApply(
               fConst('+' _)
               |(
                  fSym(A _)
                  |(
                     fSym(B _)

                  )
               )
               _
            )
            _
         )

         _
      )

\end{verbatim}

We see that a new symbol, that we denoted R for convenience, is added to the arguments list of the procedure, and this new symbol is then unified with the body of the original function. This unification will in turn be handled by the unnester, which is the next pass of the compiler.

\subsubsection{Transformation in an expression}
A statement can be transformed in an expression of the value assigned to the variable in which place a \$ was placed.
Currently only the form fun \{\$ Args\} is supported
\subsubsection{Records}
As mentioned in the description of records, the features can be left out, and are in that case implicit, with increasing integer values starting at 1.
In Oz syntax, it means that \lstinline!rec(a b c)! is transformed in \lstinline!rec(1:a 2:b 3:c)!. Of course, attention has to be paid by the programmer to avoid conflict, as implicit feature assignment does not check for explicit features'values, and conflict can arise. For example \lstinline!rec(a 1:b c)! will be transformed in \lstinline!rec(1:a 1:b 2:c)!. This behaviour is coherent with Mozart 1. %FIXME: check this is all ok.
An additional action was added in the desugar step for records: transforming records that are constants or records that have non-constant label or features. This transformation replaces fRecord that have all label, features and values constant by a fConst with as value the record reconstructed. This greatly simplifies the AST for these records, and can be done because the value of the record can be used as is in later steps, notably the opcodes generation. 
Records with non constant labels and/or features are replaced by a call to \lstinline!Boot_Record.makeDynamic!. The goal here is to leave in the AST only records with constant label and features. MakeDynamic takes two arguments. First the label of the record to construct. And second, a tuple with label '\#' and which values are alternately the features and their respective value. As a result, we replace \lstinline!Rec(F1:V1 F2:V2)! by \lstinline!{makeDynamic Rec '#'(1:F1 2:V1 3:F2 4:V2)}! .
In the AST, this replaces 
\begin{lstlisting}
  fRecord(Label 
          fColon(Feature1 Value1)|
            fColon(Feature2 Value2)|
            nil
  )
\end{lstlisting}
where Label, Feature1 and Feature2 can be fSyms, by
\begin{lstlisting}
  fApply(
         fConst(makeDynamic pos) 
         Label|
           fRecord(fConst('#' pos) 
                        Label|
                          fColon(1 Feature1)|
                          fColon(2 Value1)|
                          fColon(3 Feature2)|
                          fColon(4 Value2)|
                          nil 
           )|
           nil
  )
\end{lstlisting}
We see that we end up with a AST that only contains records with constant label and features.
Although this is not strictly a desugar step, it was added here to avoid adding a compiler pass only for this action.
\subsection{Unnester}            
The unnester will result in an AST in which there are only elementary instructions. Although the unnester works exclusively on the AST, examples given below to illustrate the unnester work will sometimes be represented in Oz code for conciseness.  The instructions susceptible to be unnested are inspected each in turn in the following subsections.
\subsubsection{Unification}
The unnesting result of a unification instruction depends on the elementary character of each side, giving these three cases:
\begin{description}
  \item[Both elementary]In this case there is nothing to do
  \item[One complex, one elementary] The unnesting is done according what the complex instruction is: 
    \begin{description}
      \item[Call] This is code of the form \lstinline!V={P E1...En}!, which needs to be transformed in the form \lstinline!{P E1...En V}!, where the variable we assign to is passed as last argument of the call.
      \item[Sequence of instructions] In the AST, the sequence of instructions is available in a hierarchy of fAnd records. We also know that the value of a sequence of instructions is the value of the last instruction in the sequence. As a consequence, the unnesting of this type of unification is done by moving the unification to the second feature of the fAnd, and recursively unnest the resulting AST.
      \item[Declaration (local)] The value of a \lstinline!local..in..end! in a unification is the value of its body. Hence, the unification \lstinline!V = local declarations in body end! is moved in side the local's body resulting in the code \lstinline!local declarations in V=body end!. This clearly requires a recursive call as the body can be a sequence of instructions, which will have to be handled as described previously.
      \item[if then else] The unification is simply moved inside each branch, and the resulting code is further unnested. For example, \lstinline!V=if B then Is1 else Is2 end! is transformed in \lstinline!if B then V=Is1 else V=Is2 end!. This needs to be recursively innested as Is1 and Is2 can be, for example,  sequences of instructions.
    \end{description}
  \item[Both complex] This is unnested by creating a new synthetic symbol, unifying it with each side, and unnesting the resulting code.
We need recursive calls, as is illustrated by the following example, also showing why this code works
\begin{lstlisting}
  3+4 = {Show}
\end{lstlisting}
It is transformed due to both sides being non-elementary, in this form in a first step:
\begin{lstlisting}
  local
     T
  in
     T = 3+4
     T = {Show}
  end
\end{lstlisting}
and finally in this form by the transformation of the assignment of a procedure call in the samem procedure call with the return variable passed as additional argument:
\begin{lstlisting}
  local
     T
  in
     T = 3+4
     {Show T}
  end
\end{lstlisting}
The order of the assignments of the first step explains why this doesn't work:
\begin{lstlisting}
  {Show} = 3+4
\end{lstlisting}

\end{description}
\subsubsection{Calls}
All arguments are unnested one by one. Elementary arguments are left untouched
Complex arguments are extracted from the arguments list by:
\begin{itemize}
  \item declaring a new symbol. This means the call is wrapped in a local..in..end, the declarations part declaring the new symbol, the body part unifying this symbol with the complex expression that was the original argument, followed by the i(to be unnested) function call with the new symbol replacing the complex argument. When a call has multiple complex arguments, this result in a local..in..end hierarchy with the depth corresponding to the number of complex arguments.
  \item unifying this new symbol with the argument
  \item replacing the argument by the new symbol in the argument list.
\end{itemize}
Here is an example of unnesting in Oz code:
\begin{lstlisting}
  {F {F2 A} B}
\end{lstlisting}
will be transformed in:
\begin{lstlisting}
  local
    NewVar
  in
    NewVar={F2 A}
    {F NewVar B}
  end
\end{lstlisting}

Of course, the function called can itself be the result of a function call, and this is handled too:
\begin{lstlisting}
  { {F1 A} B }
\end{lstlisting}
\begin{lstlisting}
  local
    NewVar
  in
    NewVar = {F1 A}
    { NewVar B }
  end
\end{lstlisting}

\subsubsection{Declarations (local)}
This is done by unnesting the body of the instruction.
\subsubsection{If then else}
The unnesting of the instruction \lstinline!if Cond then Branch1 else Branch2 end! depends on the type of Cond. 
\begin{itemize}
  \item If Cond is elementary, the unnesting is done simply by unnesting the two branched.
  \item If Cond is complex, a new symbol is created and unified with the condition Cond. This is then followed by the if then else instruction in which the complex condition is replace by the symbol newly introduced.
    \lstinline!if Cond then Is1 else Is2 end! becomes \lstinline!local S in S=Cond if S then Is1 else Is2 end!.
\end{itemize}

\subsubsection{Records}
The values in a record can be of non-elementary form, for example a function call. This has to be extracted from the record definition and replaced by a new symbol which has been unified with the initial value. The work happening on the records is very similar to the work done on (function and procedure) calls. Rather than working on the procedure arguments, we work on the pairs feature-value and only handle the value. 
Here is an example where the value of a feature is an if expression.
\begin{lstlisting}
   R=rec(a:if B then 2 else V end b:2)
\end{lstlisting}
will be transformed in
\begin{lstlisting}
  local
    NewVar
  in
    NewVar=if B then 2 else V end
    R=rec(a:NewVar b:2)
  end
\end{lstlisting}
And this requires a new unnesting step (see the unnesting of unifications) resulting in 
\begin{lstlisting}
  local
    NewVar
  in
    if B then NewVar=2 else NewVar=V end
    R=rec(a:NewVar b:2)
  end
\end{lstlisting}



\subsection{Globaliser}            
This pass handles the global variables of a procedure, that is, variables used by a procedure that does not declare it (implying that the declaration is done at an upper level. It is interesting to note that the concept of global variables is only defined for procedures.
Just as locals attached an environment to their body, procedures attach their own unique procId to their arguments, declarations and body.
Variables declared by a procedure, including formal parameters, get assigned the procId of said procedure.

Special attention has to be paid to nested procedure declarations. For example
\begin{lstlisting}
A=1
proc {P1}                 % A is global to P1
   B=2
   P2
in
   proc {P2}              % A and B are globals to P2
      P3 
   in
      proc {P3}           % A and B are globals to P3
         C
      in
         {Show A}
         {Show B}
         {Show C}
      end
      {P3}
   end
   {P2}
end
\end{lstlisting}

The globals of procedures in the code have to be determined from the inside to the outside: determine the globals of the deepest nested procedure, and go up. 
We see that the globals of a procedure are:
\begin{itemize}
  \item all variables it uses directly (i.e. not in a procedure definition)
  \item plus the globals of all the procedures it defines
  \item minus the variables it declares itself
\end{itemize}

In the example we see that P3 uses 2 variables declared by another procedure: A and B. 
P2 does not directly use any variable, but it defines P3 which itself has 2 global variables. Those two variables are thus also globals to P2
P1 also does not use any variable directly, but it gets 2 globals from P2, of which it declares one: B. P1 indeed only has one global: A.

In the abstract syntax tree, we replace symbols corresponding to a global variables by a new local symbol of type 'newlocal' referencing the symbol for this variables in the parent proc which might itself be a 'newlocal' symbol, and so on until we reach the level where the variables is declared.

In the abstract syntax tree, fProc is replaced by fDefineProc taking 1 additional argument: the newly created local variables referencing a variables in the surrounding procedure. This additional information will enable the code generator handle global variables.

Handling globals correctly requires to handle numerous cases, and determining the algorithm to use requires some care, as illustrated by these examples:

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
   end
end
\end{lstlisting}
In this case, A is global to P3 and it creates a new local symbol that will reference a symbol local to P2. But for P2, A is also global, and this situation needs to trigger a new local creation in P2, which will be referenced by the new local in P3. Same thing in P1.

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      {Show A}  % Use A before a defined procedure needs it
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
   end
end
\end{lstlisting}

In this case, the traversal of the AST will first analyse {Show A} and create a new local symbol for the variables A in P1. P2 is visited later and will create a new local to reference a symbol local to P2, which is also a new local as A is global to P1. But this new local has already been created, and this situation must not trigger the creation of a new local symbol for A. Rather, the new local symbol to P2 must reference the new local symbol to P1. We see that the new local variables created in a procedure definition may have to be modified by its parent procedure.

Of course, the inverse has to be handled too, i.e. a new local is created for a defined proc, and reused later:

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
      {Show A}  % Use A after a nested proc triggered the creation of a new local.
   end
end
\end{lstlisting}


Of course, a new local symbol must not be created when it would represent a locally declared variables, as in this case:

\begin{lstlisting}
local
   A
in
   proc {P1}
      P2
   in
      proc {P2}
         P3
      in
         proc {P3}
            {Show A}
         end
         {P3}
      end
   end
   {Show A}  % A is defined locally, and the nested procs defined should not trigger the creation of a new local, but P1's new local for A should reference the A declared by the local .. in .. end
end
\end{lstlisting}

Two procs defined at the same level must have have their respective new local symbols reference the same symbol at the parent level (whether it be a new local symbol created at the parent level or the existing symbol of a declared variable at the parent level). In this example, P2 and P3 each create a new local for A and both trigger the creation of a new local in P1 for A. But only one new local should be created in P1 for A, and both locals in P2 and P3 should reference it.


Example V:
\begin{lstlisting}
local
   A
in
   proc {P1}
      P2 P3
   in
      proc {P2}
         {Show A}
      end
      proc {P3}
          {Show A}
      end
      {P2}
      {P3}
   end
   {Show A}  % Use A after a nested proc triggered the creation of a new local.
end
\end{lstlisting}

In the end, the algorithm used is the following.
Each call of the globaliser takes the AST sub tree and an additional Params argument with three fields:
\begin{itemize}
   \item the current procId
   \item a list of global variables already seen in the current procId
   \item a list, each item being the list of new local symbols created for the global with same index in the previous list.
\end{itemize}

The globalise handles the following nodes:
\begin{description}
   \item[fProc] When handling a fProc, the globaliser call gets the informations from the parent in its Params argument. Because we enter a new level of procedure nesting, a NewParams is initialised to be passed to recursive globaliser calls handling children nodes. NewParams is initialised with a new procId and two empty lists. 
      The symbol of the procedure itself gets the procId of its parent, found in Params. The arguments of the procedure get the procId of the currently handled procedure, found in NewParams. At that time the globaliser function is called recursively on the children nodes, with NewParams as additional argument.
      Once all children have been traversed, the list of their globals and their respective newly created locals is found in NewParams.  We have multiple new locals corresponding to one global in the case of 2 siblings procedure definitions referring the same global as in example V. 
      Globals whose procId match the NewParams' procId are ignored, as those are variables that are declared at this level, and the chain of references must stop here for these variables.
      For each remaining global, we look at the list of their locals. 
      If one of these has the procId matching the current level, all other locals are changed to reference this one. This is because this specific symbol has been created by a direct use at this level, and we will use this one as the new localised symbol at this level. Then we push the global on the parent's globals list and add the local symbol to its corresponding list of new locals (those lists are found in Params)
      If none of the locals has the current level's procId, it means the global variables has only been used in nested procedure definitions. We need to create a new local symbol for that variable at the current level that will be referenced by the existing locals. Finally, we push the global and the new local symbol we created to the parent. 
   \item[fSym] If the symbol in this fSym has the current procId found in NewParams, keep it as is, do not change it! 
      If its procId is different from the current procId, this fSym represents a global variable for the current level. We look in Params if a local variable with the current procId has already been created for this global variable. If yes we reuse it, else We define a new symbol, having the current procId and referencing the symbol initially in fSym. We modify the fSym to use that newly created symbol. Finally, push the global and its local to the parent.
      
\end{description}



\subsection{CodeGen}
\subsubsection{Register allocation}
It is CodeGen that allocates registers. For the moment only Y registers.
When a Symbol of type global is found, it is handled differently than local symbols.
For a procedure definition, we initialise the global variables with arrayFill( y(yindex)). \fxnote{handle multiple levels of nesting}
For globals, a g() is issued in place of a y(). \fxnote{is this working yet?}
\subsubsection{If then else}
Rather than give convoluted explanations, it is easier to look at the code generating the opcodes for the record \lstinline!fBoolCase(FSym TrueCode FalseCode Pos)! found in the AST for an if..then..else instruction:
\begin{lstlisting}
  move({GenCodeInt FSym Params} x(0))|
  condBranch(x(0) ElseLabel ErrorLabel)|
  %---- true ----
  {GenCodeInt TrueCode Params}|
  branch(EndLabel)|
  %---- error ----
  lbl(ErrorLabel)|
  move(k(badBooleanInIf) x(0))|
  tailCall(k(Exception.raiseError) 1)|
  %---- else ----
  lbl(ElseLabel)|
  {GenCodeInt FalseCode Params}|
  % ---- end ----
  lbl(EndLabel)|nil
\end{lstlisting}
It first moves the symbol containing the boolean to be tested in a X register, which is then used in a condBranch opcode, takin gas additional arguments the labels identifying the start of the else branch, and the start of the error handling code. The ``then'' branch directly follows the condBranch. After that come the error code and the else branch. Note that at the end of the ``then'' branch, a branch opcode is needed to jump over the code corresponding to the else branch and the error handling.

\subsubsection{Records}
At this step, the only place we can find records are in the right hand side of unifications, i.e. the second features of fEq. 
The virtual machine requires, for performance reasons, the compiler to generate different opcodes for cons, tuples and records.
If the record is a cons, i.e. a record with label '|' and with exactly two features with values 1 and 2, we need to issue a makeConsUnify(DestReg). If the arity is made of numbers only, i.e. we have the arity of a tuple, we need to issue a makeTupleUnify(k(Label) FeaturesList DestReg) instruction. Else we need to issue a makeRecordUnify(k(Arity) FeaturesList DestReg) instruction.
The function CompilerSupport.makeArity is used to build the arity for use in makeRecordUnify. It takes as argument the label and a list of pairs \lstinline!Label#Value!. If the labels passed in the list are labels of a tuple, it returns false, else it returns the constructed arity. It is based on the value returned by makeArity that the compiler decides which instruction is issued.
The featuresList has to be ordered according to the features. This is important, as the makeConsUnify/makeTupleUnify/makeRecordUnify instructions have to be followed by arrayFill instructions, one for each value, in the same order as the features passed to the instruction.

As an illustration, this unification of a variable with a record having one value not constant:
\begin{lstlisting}
R=rec(a:A b:2)
\end{lstlisting}
results in this AST
\begin{lstlisting}
   fEq(
      fSym('R' pos)
      fRecord(
         fAtom(rec pos)
         |(
            fColon(
               fAtom(a pos)
               fSym('A' pos)
            )
            |(
               fColon(
                  fAtom(b pos)
                  fInt(2 pos)
               )
               nil
            )
         )
      )
      pos
   )
\end{lstlisting}
and, if we note y(VAR) the register assigned to VAR by the compile, it results in these opcodes being generated:

\begin{lstlisting}
   makeRecordUnify( k(<Arity rec(a b)>) 2 y(R) )
   arrayFill(y(A))
   arrayFill(k(2)) 
\end{lstlisting}

\section{Tests}
\subsection{Helper functions tests}
Some helper functions are tested specifically by Oz code, checking that the result returned by the function is correct. This is simply done by calling the function in Figure~\ref{fig:helperstestsfunction} with the first argument the function call we want to test, and the expected result as second argument as illustrated in Figure~\ref{fig:helperstestsexample} where the function UnWrapFAnd is tested.
The only downside to this approach is that helper functions have to be exported for them to be available in the test code..

\begin{figure}
\begin{lstlisting}
   proc {Equals Result Expected}
      if Result\=Expected then
         %Show error information
         raise unexpectedResult end
      end
      {System.printInfo '.'}
   end
\end{lstlisting}
\label{fig:helperstestsfunction}
\caption{HelpersTests function}
\end{figure}

\begin{figure}
\begin{lstlisting}
   {Equals 
      {Compile.unWrapFAnd fAnd(first fAnd(second third) )}
      first|second|third|nil }
\end{lstlisting}
\label{fig:helperstestsexample}
\caption{Helper test example}
\end{figure}
\subsection{Compiler tests}
Tests are defined by three files each:
\begin{itemize}
  \item the oz code to compile and execute
  \item the expected stdout
  \item the expected stderr
\end{itemize}
All these files are put under the tests/definitions directory.

A TestRunner script in oz loads the code found in the file whose path is passed as argument, parses it, gives the AST to the compiler which returns the opcodes which are assembled and executed. 
A shell script iterates over all tests and for each execute the TestRunner, puts the stdout and stderr in result files under the tests/results directory, compares their content with the expected results and in case of difference can open a diffviewer (like vimdiff).
All test code has a preamble comment describing what case this test covers.
If the test code contains the comment line
\begin{lstlisting}
%-- SKIP TEST-- 
\end{lstlisting}
the test is skipped. This was implemented to be able to run tests even if one of them was temporarily not passing.

Adding a test is very simple:
\begin{enumerate}
  \item put the code to compile and execute in an .oz file under tests/descriptions
  \item put the expected output in a file with same name but with extension .out
  \item put the expected error output in a file with same name but with extension .err
\end{enumerate}
Once this is done, the new test will be included in the next run.
It is also possible to run one individual test. In this case, the test is run even if it is maked as to be skipped when running the full test suite. This helps writing tests for cases needed to be supported in the future, but that should not make the full test suite fail.
\section{Performance}
Possibly include some measure?



\end{document}
